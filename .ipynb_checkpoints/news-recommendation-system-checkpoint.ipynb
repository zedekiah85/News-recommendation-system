{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install numpy==1.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Getting these values because: This can happen due to the presence of \"missing\" values that aren't technically NaN (the value used by Pandas to represent missing data), but are instead represented by other placeholder values, such as empty strings (''), whitespace, or special markers like None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example to show the presence of missing values\n",
    "df[209217:209224]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this case we have the some missing values in the author column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking if there are any other mssing values\n",
    "df.replace(\"\", pd.NA, inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Percentage of missing values\n",
    "missing_percentage = df.isnull().mean()*100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Replacing the author column column with unknown\n",
    "df['authors']=df['authors'].replace(pd.NA, \"unknown\")\n",
    "\n",
    "#Drop the remaining columns with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Checking if there are any other missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropping the date and the link column\n",
    "\n",
    "df.drop(columns=['link', 'date'], inplace=True)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -U pip setuptools wheel\n",
    "#!pip install -U spacy\n",
    "\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combining the headline column and the short description column\n",
    "#df['text'] = df['headline'] + \" \" + df['short_description']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropping the headline and short description column\n",
    "df.drop(['authors'], inplace=True, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.head(1000)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a language preprocessing pipeline for the dataset\n",
    "\n",
    "#Processing the text in batches using spacy's nlp.pipelines for faster execution\n",
    "def preprocess_text_column(df, column):\n",
    "    texts = df[column].astype(str).tolist() # Converting columns to alist of strings\n",
    "    processed_texts = []\n",
    "    \n",
    "    \n",
    "    #Using nlp for batch processing\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, disable=['parser'], n_process=-1):\n",
    "        #Change text to lowercase\n",
    "        doc = nlp(doc.text.lower())\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        processed_texts.append(\" \".join(tokens))\n",
    "        \n",
    "    df[column]=processed_texts\n",
    "    return df\n",
    "        \n",
    "#Columns to process\n",
    "columns_to_process = [\"headline\", \"category\", \"short_description\"]\n",
    "\n",
    "for column in columns_to_process:\n",
    "    df = preprocess_text_column(df, column)\n",
    "    \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_json('data.json', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df.to_csv('data.csv', index=False, header=True, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Text Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using one hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#Extracting the text column\n",
    "text = df[['headline', 'category', 'short_description']]\n",
    "\n",
    "#Creating an instance of the onehotencoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "pipeline1 = Pipeline(steps=[('OneHotEncoder', OneHotEncoder())])\n",
    "\n",
    "\n",
    "#Fitting and transforming the genres column\n",
    "text_encoded = pipeline1.fit_transform(text.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipeline1, \"pipeline1.joblib\")\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#Creating a instance of the NearestNeighbors class\n",
    "recommender = NearestNeighbors(metric='cosine')\n",
    "\n",
    "#Fitting the encoded genres to the recommender\n",
    "recommender.fit(text_encoded.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Making Recommendations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of recommendations to return\n",
    "num_recommendations = 5\n",
    "\n",
    "# Getting the recommendations\n",
    "_, recommendations = recommender.kneighbors(text_encoded.toarray(), n_neighbors=num_recommendations)\n",
    "\n",
    "# Ensure the indices are within bounds of the DataFrame\n",
    "recommendations = [index for index in recommendations[0] if index < len(df)]\n",
    "\n",
    "# Extracting the text from the recommendations\n",
    "if recommendations:\n",
    "    recommended_text_titles = df.iloc[recommendations][['category', 'headline', 'short_description']]\n",
    "else:\n",
    "    recommended_text_titles = pd.DataFrame(columns=['category', 'headline', 'short_description'])  # Empty if no valid indices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "recommended_text_titles\n",
    "\n",
    "text_encoded.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Saving to a vector database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu\n",
    "#!pip install numpy==1.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "#Create a sample numpy array\n",
    "dimension = 2024\n",
    "num_vectors = 3000\n",
    "\n",
    "#Create a FAISS index\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "#Add vectors to the index\n",
    "index.add(text_encoded.toarray())\n",
    "\n",
    "#Save the index to disk\n",
    "faiss.write_index(index, 'vector_database.index')\n",
    "print(f'Index saved with {index.ntotal} vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Query from database\n",
    "\n",
    "#Load the saved index\n",
    "index = faiss.read_index('vector_database.index')\n",
    "\n",
    "#Create a query vector/ multiple query vectors\n",
    "text_index = 0\n",
    "num_queries = 1\n",
    "query_vectors = text_encoded[text_index].toarray()\n",
    "\n",
    "#Perform the search\n",
    "k=5 #Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_vectors, k)\n",
    "\n",
    "#Print results\n",
    "for i in range(num_queries):\n",
    "    print(f'Query {i}:')\n",
    "    for j in range(k):\n",
    "        print(f\"Neighbor {j}: Index {indices[i][j]}, Distance {distances[i][j]}\")\n",
    "\n",
    "#Create a query vector\n",
    "num_queries = 1\n",
    "query_vector = text_encoded[5].toarray()\n",
    "\n",
    "#Perform the search\n",
    "k=5 #Number of nearest neighbors to retrieve\n",
    "distances, indices = index.search(query_vector, k)\n",
    "\n",
    "#Print results\n",
    "print(f'Query:')\n",
    "for j in range (k):\n",
    "    print(f'Neighbor {j}: Index {indices[0][j]}, Distance {distances[0][j]}')\n",
    "    \n",
    "    #Acces the corresponding Dataframe rows using the retrieved indices\n",
    "    neighbor_rows = df.iloc[indices[0]]\n",
    "    print(neighbor_rows)\n",
    "\n",
    "# prompt: How to get the text representation of query_vector = text_encoded[0].toarray()  # Assuming you want to query with the first vector\n",
    "\n",
    "query_vector_text = df.iloc[0][['headline', 'category', 'short_description']]\n",
    "print(query_vector_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Using TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vector_df = vectorizer.fit_transform(df['headline'].values).toarray()\n",
    "vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "vectorizer = TfidfVectorizer()\n",
    "vector_df = vectorizer.fit_transform(df['category'].values).toarray()\n",
    "\n",
    "# Get the words (features) from the vectorizer\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for easier handling\n",
    "tfidf_df = pd.DataFrame(vector_df, columns=words)\n",
    "\n",
    "# Set the number of top words to retrieve\n",
    "top_n = 5  # You can change this to any number of top words you'd like\n",
    "\n",
    "# Extract the top words per document\n",
    "top_words_per_doc = {}\n",
    "\n",
    "for idx, row in tfidf_df.iterrows():\n",
    "    # Get top n words based on TF-IDF score for the current document\n",
    "    top_words = row.nlargest(top_n).index.tolist()\n",
    "    top_words_per_doc[f\"Document {idx+1}\"] = top_words\n",
    "\n",
    "# Display the top words for each document\n",
    "for doc, top_words in top_words_per_doc.items():\n",
    "    print(f\"{doc}: {', '.join(top_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combine 'headline' and 'short_description' for vectorization\n",
    "df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "\n",
    "# Vectorize the combined text\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorized_text = vectorizer.fit_transform(df['combined_text'].values)\n",
    "\n",
    "# Calculate cosine similarity on the vectorized text\n",
    "cosine_sim = cosine_similarity(vectorized_text, vectorized_text)\n",
    "\n",
    "# Define recommendation function with category filtering\n",
    "def get_recommendations(category, cosine_sim=cosine_sim, df=df):\n",
    "    # Check if the article with the specified title and category exists\n",
    "    matching_rows = df['category'] == category\n",
    "    if matching_rows.empty:\n",
    "        print(\"No article found with the specified category.\")\n",
    "        return\n",
    "    \n",
    "    # Get the index of the article that matches the category\n",
    "    idx = matching_rows.index[0]\n",
    "\n",
    "    # Get pairwise similarity scores for all articles with the selected article\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort articles by similarity score in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 5 most similar articles within the same category\n",
    "    recommendations = [\n",
    "        i for i in sim_scores if df.iloc[i[0]]['category'] == category and i[0] != idx\n",
    "    ][:5]\n",
    "\n",
    "    # Display recommended articles\n",
    "    if recommendations:\n",
    "        for i in recommendations:\n",
    "            #print(f\"Title: {df.iloc[i[0]]['headline']}\")\n",
    "            print(f\"Category: {df.iloc[i[0]]['category']}\")\n",
    "            print(f\"Description: {df.iloc[i[0]]['short_description']}\\n\")\n",
    "    else:\n",
    "        print(\"No similar articles found within the same category.\")\n",
    "\n",
    "# Test the recommendation function with a specific headline and category\n",
    "get_recommendations(\"comedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "\n",
    "# Save the cosine similarity matrix\n",
    "joblib.dump(cosine_sim, 'cosine_sim.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Combine 'headline' and 'short_description' for vectorization\n",
    "df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "\n",
    "# Load the vectorizer and cosine similarity matrix\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "cosine_sim = joblib.load('cosine_sim.pkl')\n",
    "\n",
    "# Define recommendation function with category filtering\n",
    "def get_recommendations(category, cosine_sim=cosine_sim, df=df):\n",
    "    # Check if the article with the specified title and category exists\n",
    "    matching_rows = df['category'] == category\n",
    "    if matching_rows.empty:\n",
    "        print(\"No article found with the specified category.\")\n",
    "        return\n",
    "    \n",
    "    # Get the index of the article that matches the category\n",
    "    idx = matching_rows.index[0]\n",
    "\n",
    "    # Get pairwise similarity scores for all articles with the selected article\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort articles by similarity score in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the top 5 most similar articles within the same category\n",
    "    recommendations = [\n",
    "        i for i in sim_scores if df.iloc[i[0]]['category'] == category and i[0] != idx\n",
    "    ][:5]\n",
    "\n",
    "    # Display recommended articles\n",
    "    if recommendations:\n",
    "        for i in recommendations:\n",
    "            #print(f\"Title: {df.iloc[i[0]]['headline']}\")\n",
    "            print(f\"Category: {df.iloc[i[0]]['category']}\")\n",
    "            print(f\"Description: {df.iloc[i[0]]['short_description']}\\n\")\n",
    "    else:\n",
    "        print(\"No similar articles found within the same category.\")\n",
    "\n",
    "# Test the recommendation function with a specific headline and category\n",
    "get_recommendations(\"environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import faiss  # if required\n",
    "#from your_model_file import load_your_model, recommend_news  # assuming these functions are defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Title and description\n",
    "st.title(\"News Recommendation System\")\n",
    "st.write(\"Get personalized news recommendations based on your preferences.\")\n",
    "\n",
    "# User input (e.g., interest area)\n",
    "user_input = st.text_input(\"Enter your interests or topics:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@st.cache\n",
    "def load_model():\n",
    "    return load_your_model(\"model_path.pkl\")  # replace with actual loading function\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "if st.button(\"Recommend News\"):\n",
    "    recommendations = recommend_news(model, user_input)\n",
    "    for rec in recommendations:\n",
    "        st.write(f\"Title: {rec['title']}\\nDescription: {rec['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 32526,
     "sourceId": 4243451,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
